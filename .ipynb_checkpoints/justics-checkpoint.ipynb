{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Creating new text file\n",
      "1\n",
      "Creating new text file\n",
      "data/justice/pdfs/17-0025 (UC Online)_1.pdf\n",
      "hi!!\n",
      "&&&&\n",
      "2\n",
      "Creating new text file\n",
      "data/justice/pdfs/17-0030 (Education)_0.pdf\n",
      "hi!!\n",
      "&&&&\n",
      "3\n",
      "Creating new text file\n",
      "data/justice/pdfs/17-0031 (California Economic Rights)_0.pdf\n",
      "hi!!\n",
      "&&&&\n",
      "4\n",
      "Creating new text file\n",
      "data/justice/pdfs/17-0032 (Tax Credits).pdf\n",
      "hi!!\n",
      "&&&&\n",
      "5\n",
      "Creating new text file\n",
      "data/justice/pdfs/17-0033 (Gas and Car Tax)_0.pdf\n",
      "hi!!\n",
      "&&&&\n",
      "6\n",
      "Creating new text file\n",
      "data/justice/pdfs/17-0034 (Tax and Bond Revenue).pdf\n",
      "hi!!\n",
      "&&&&\n",
      "7\n",
      "Creating new text file\n",
      "data/justice/pdfs/17-0034 (Tax and Bond Revenue)_0.pdf\n",
      "hi!!\n",
      "&&&&\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "\n",
    "\n",
    "PDF_PATH = os.path.join('data', 'justice','pdfs')\n",
    "PDF_NAMES = os.listdir(PDF_PATH)\n",
    "CORPUS_PATH = os.path.join('data','justice','txt')\n",
    "# CORPUS_PATH = os.path.join('data', 'austen-brontÃ«-split')\n",
    "# print(PDF_PATH)\n",
    "# print(PDF_NAMES)\n",
    "# print(CORPUS_PATH)\n",
    "\n",
    "#make a list of pdf files names\n",
    "#go through each of them and write them to the same directory as a text file \n",
    "\n",
    "pdffilenames = sorted([os.path.join(PDF_PATH, fn) for fn in os.listdir(PDF_PATH)])\n",
    "# print(pdffilenames)\n",
    "\n",
    "\n",
    "def pdfparser(data):\n",
    "\n",
    "    fp = open(data, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    # Process each page contained in the document.\n",
    "\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "\n",
    "#     print(data)\n",
    "    return data\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# pdfparser(CORPUS_PATH)  \n",
    "\n",
    "\n",
    "def pdfConverter(arrayOfFullFileNames, arrayOfPDFNames, destPath):\n",
    "    for i in range(len(arrayOfFullFileNames)):\n",
    "        print(i)\n",
    "        print('Creating new text file') \n",
    "        if not str(arrayOfPDFNames[i]).startswith('.'):\n",
    "            name = str(arrayOfPDFNames[i]) + '.txt'\n",
    "#         print('&&&&')\n",
    "#         print(name)\n",
    "#         print('*****')\n",
    "#         print(destPath)\n",
    "            print(arrayOfFullFileNames[i])\n",
    "\n",
    "            try:\n",
    "#             print(pdfparser())\n",
    "                content = pdfparser(arrayOfFullFileNames[i])\n",
    "#                 print('below is the content')\n",
    "#                 print(content)\n",
    "                contenttwo = str(content)\n",
    "                print('hi!!')\n",
    "#                 print(type(contenttwo))\n",
    "#                 print(contenttwo)\n",
    "                print('&&&&')\n",
    "                file = open(destPath+name,'w')\n",
    "                file.write(str(content))\n",
    "                file.close()\n",
    "\n",
    "            except:\n",
    "                print('Something went wrong! Can\\'t tell what?')\n",
    "                sys.exit(0) # quit Python\n",
    "        \n",
    "pdfConverter(pdffilenames,PDF_NAMES,CORPUS_PATH) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/justice/txt\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import decomposition\n",
    "print(CORPUS_PATH)\n",
    "filenames = sorted([os.path.join(CORPUS_PATH, fn) for fn in os.listdir(CORPUS_PATH)])\n",
    "print(filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = text.CountVectorizer(input='filename', stop_words='english', min_df=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-8cb0608b6b02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiaoweirwang/miniconda3/envs/numlinalg/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiaoweirwang/miniconda3/envs/numlinalg/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    782\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "print(filenames)\n",
    "dtm = vectorizer.fit_transform(filenames).toarray()\n",
    "print(dtm)\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(vocab))\n",
    "print(vectorizer)\n",
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813, 2902)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "num_top_words = 20\n",
    "clf = decomposition.NMF(n_components=num_topics, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doctopic = clf.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['know', 'think', 'like', 'man', 'good', 'say', 'does', 'said', 'oh', 'quite', 'sure', 'yes', 'thing', 'believe', 'way', 'love', 'young', 'people', 'better', 'thought'], ['mr', 'knightley', 'rochester', 'elton', 'man', 'did', 'think', 'evening', 'business', 'away', 'wickham', 'gentleman', 'time', 'dear', 'wife', 'told', 'gentlemen', 'brother', 'information', 'quite'], ['marianne', 'elinor', 'willoughby', 'sister', 'jennings', 'mother', 'colonel', 'dashwood', 'mrs', 'brandon', 'time', 'moment', 'heart', 'little', 'barton', 'soon', 'mind', 'spirits', 'middleton', 'lady'], ['elizabeth', 'darcy', 'bingley', 'mr', 'sister', 'bennet', 'wickham', 'miss', 'soon', 'lydia', 'friend', 'room', 'gardiner', 'netherfield', 'little', 'jane', 'family', 'father', 'lizzy', 'said'], ['mrs', 'did', 'quite', 'elton', 'room', 'come', 'good', 'thing', 'said', 'weston', 'jennings', 'came', 'day', 'party', 'palmer', 'fairfax', 'colonel', 'know', 'went', 'heard'], ['like', 'room', 'door', 'night', 'long', 'saw', 'thought', 'looked', 'house', 'time', 'eyes', 'day', 'face', 'rochester', 'light', 'heard', 'little', 'stood', 'hand', 'hour'], ['sir', 'rochester', 'yes', 'shall', 'adele', 'don', 'little', 'good', 'john', 'said', 'll', 'mr', 'say', 'eyre', 'room', 'thornfield', 'like', 'lady', 'old', 'ingram'], ['jane', 'fairfax', 'love', 'little', 'think', 'oh', 'heard', 'heart', 'mother', 'soon', 'make', 'feel', 'voice', 'campbell', 'bates', 'does', 'god', 'away', 'elton', 'wish'], ['miss', 'woodhouse', 'bates', 'quite', 'sure', 'young', 'fairfax', 'oh', 'lady', 'say', 'room', 'smith', 'temple', 'bessie', 'great', 'came', 'just', 'think', 'heard', 'ladies'], ['john', 'man', 'good', 'father', 'house', 'mother', 'years', 'young', 'dashwood', 'life', 'old', 'family', 'wife', 'home', 'woman', 'fortune', 'sisters', 'thousand', 'make', 'st'], ['little', 'graham', 'bretton', 'papa', 'dr', 'lucy', 'like', 'polly', 'home', 'child', 'paulina', 'don', 'think', 'thought', 'eyes', 'bassompierre', 'john', 'said', 'snowe', 'father'], ['lady', 'collins', 'elizabeth', 'catherine', 'bennet', 'charlotte', 'family', 'mr', 'young', 'make', 'mrs', 'ladyship', 'ladies', 'daughter', 'rosings', 'soon', 'bourgh', 'little', 'mother', 'william'], ['monsieur', 'little', 'english', 'mdlle', 'frances', 'mademoiselle', 'vous', 'est', 'said', 'pelet', 'hand', 'pupils', 'eyes', 'reuter', 'henri', 'thought', 'time', 'school', 'french', 'good'], ['harriet', 'emma', 'elton', 'mr', 'friend', 'woodhouse', 'knightley', 'martin', 'think', 'good', 'thing', 'little', 'smith', 'thought', 'mind', 'woman', 'hartfield', 'just', 'great', 'having'], ['did', 'know', 'heart', 'felt', 'look', 'thought', 'told', 'knew', 'night', 'believe', 'time', 'day', 'saw', 'feelings', 'think', 'love', 'came', 'words', 'paul', 'make'], ['madame', 'beck', 'dr', 'knew', 'little', 'door', 'day', 'paul', 'old', 'night', 'good', 'english', 'pupils', 'thought', 'hand', 'like', 'pelet', 'stood', 'came', 'john'], ['said', 'come', 'shall', 'know', 'tell', 'think', 'john', 'cried', 'looked', 'did', 'replied', 'let', 'asked', 'answer', 'door', 'yes', 'answered', 'quite', 'want', 'eyes'], ['edward', 'elinor', 'lucy', 'ferrars', 'dashwood', 'sister', 'think', 'mother', 'say', 'time', 'sure', 'brother', 'soon', 'thing', 'make', 'colonel', 'great', 'really', 'brandon', 'engagement'], ['emma', 'weston', 'mr', 'knightley', 'frank', 'churchill', 'thing', 'little', 'father', 'woodhouse', 'great', 'fairfax', 'time', 'hartfield', 'body', 'randalls', 'soon', 'mrs', 'dear', 'bates'], ['shall', 'letter', 'time', 'dear', 'day', 'soon', 'read', 'come', 'know', 'say', 'long', 'hope', 'away', 'sure', 'till', 'oh', 'good', 'going', 'write', 'lydia']]\n"
     ]
    }
   ],
   "source": [
    "topic_words = []\n",
    "for topic in clf.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "    topic_words.append([vocab[i] for i in word_idx])\n",
    "\n",
    "print(topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doctopic = doctopic / np.sum(doctopic, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "novel_names = [] \n",
    "\n",
    "for fn in filenames:\n",
    "    basename = os.path.basename(fn)\n",
    "    name, ext = os.path.splitext(basename)\n",
    "    name = name.rstrip('0123456789')\n",
    "    novel_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Austen_Emma', 'Austen_Pride', 'Austen_Sense', 'CBronte_Jane', 'CBronte_Professor', 'CBronte_Villette']\n",
      "[[ 0.          0.00506178  0.00985299 ...,  0.          0.22786104  0.        ]\n",
      " [ 0.12041777  0.25543906  0.         ...,  0.          0.23353457\n",
      "   0.08181409]\n",
      " [ 0.12093734  0.28432916  0.         ...,  0.          0.19369504\n",
      "   0.0502726 ]\n",
      " ..., \n",
      " [ 0.0573295   0.          0.         ...,  0.08810671  0.          0.        ]\n",
      " [ 0.064562    0.03481487  0.         ...,  0.0627449   0.          0.06113009]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.12133201]]\n",
      "Austen_Emma: 9 8 18\n",
      "Austen_Pride: 1 18 8\n",
      "Austen_Sense: 1 18 8\n",
      "CBronte_Jane: 9 8 18\n",
      "CBronte_Professor: 1 18 4\n",
      "CBronte_Villette: 9 18 4\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-5d3cf556cc87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtop_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoctopic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtop_topics_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnovels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_topics_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "novel_names = np.asarray(novel_names)\n",
    "doctopic_orig = doctopic.copy()\n",
    "num_groups = len(set(novel_names))\n",
    "# print(num_groups)\n",
    "# print(doctopic.copy)\n",
    "doctopic_grouped = np.zeros((num_groups, num_topics))\n",
    "\n",
    "# print(doctopic_grouped)\n",
    "\n",
    "# for i, name in enumerate(sorted(set(novel_names))):\n",
    "#     doctopic_grouped[i, :] = np.mean(doctopic[novel_names == name, :], axis=0)\n",
    "\n",
    "#     doctopic = doctopic_grouped\n",
    "\n",
    "novels = sorted(set(novel_names))\n",
    "print(novels)\n",
    "print(doctopic)\n",
    "for i in range(len(doctopic)):\n",
    "    top_topics = np.argsort(doctopic[i,:])[::-1][0:3]\n",
    "    top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "    print(\"{}: {}\".format(novels[i], top_topics_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: know think like man good say does said oh quite sure yes thing believe way\n",
      "Topic 1: mr knightley rochester elton man did think evening business away wickham gentleman time dear wife\n",
      "Topic 2: marianne elinor willoughby sister jennings mother colonel dashwood mrs brandon time moment heart little barton\n",
      "Topic 3: elizabeth darcy bingley mr sister bennet wickham miss soon lydia friend room gardiner netherfield little\n",
      "Topic 4: mrs did quite elton room come good thing said weston jennings came day party palmer\n",
      "Topic 5: like room door night long saw thought looked house time eyes day face rochester light\n",
      "Topic 6: sir rochester yes shall adele don little good john said ll mr say eyre room\n",
      "Topic 7: jane fairfax love little think oh heard heart mother soon make feel voice campbell bates\n",
      "Topic 8: miss woodhouse bates quite sure young fairfax oh lady say room smith temple bessie great\n",
      "Topic 9: john man good father house mother years young dashwood life old family wife home woman\n",
      "Topic 10: little graham bretton papa dr lucy like polly home child paulina don think thought eyes\n",
      "Topic 11: lady collins elizabeth catherine bennet charlotte family mr young make mrs ladyship ladies daughter rosings\n",
      "Topic 12: monsieur little english mdlle frances mademoiselle vous est said pelet hand pupils eyes reuter henri\n",
      "Topic 13: harriet emma elton mr friend woodhouse knightley martin think good thing little smith thought mind\n",
      "Topic 14: did know heart felt look thought told knew night believe time day saw feelings think\n",
      "Topic 15: madame beck dr knew little door day paul old night good english pupils thought hand\n",
      "Topic 16: said come shall know tell think john cried looked did replied let asked answer door\n",
      "Topic 17: edward elinor lucy ferrars dashwood sister think mother say time sure brother soon thing make\n",
      "Topic 18: emma weston mr knightley frank churchill thing little father woodhouse great fairfax time hartfield body\n",
      "Topic 19: shall letter time dear day soon read come know say long hope away sure till\n"
     ]
    }
   ],
   "source": [
    "for t in range(len(topic_words)):\n",
    "    print(\"Topic {}: {}\".format(t, ' '.join(topic_words[t][:15])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
